from fastapi import APIRouter, UploadFile, File, HTTPException, Query
import os
import pandas as pd
import urllib.parse  # ğŸŒŸ å¿…å‚™ï¼šè™•ç†ä¸­æ–‡å“åè½‰ç¶²å€ç·¨ç¢¼

# ğŸŒŸ åŒ¯å…¥æ‰“å¡ç³»çµ±
try:
    from services.stats_api import log_action
except ImportError:
    def log_action(name): pass

router = APIRouter()
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)

# ğŸŒŸ å°ˆå±¬æ–¼ã€Œæœå°‹ç³»çµ±ã€çš„æª”æ¡ˆ
SEARCH_DB_NAME_FILE = os.path.join(DATA_DIR, "search_db_name.txt")

def get_search_db_path():
    # è‡ªå‹•åµæ¸¬ä¸Šå‚³çš„æ˜¯ csv é‚„æ˜¯ xlsx
    for ext in ['.csv', '.xlsx', '.xls']:
        p = os.path.join(DATA_DIR, f"search_data{ext}")
        if os.path.exists(p): 
            return p
    return None

# å…¨åŸŸè®Šæ•¸ï¼šæœå°‹ç³»çµ±å°ˆç”¨è¨˜æ†¶é«”å¿«å–
_search_cache = None
_search_mtime = 0

def load_search_db():
    global _search_cache, _search_mtime
    db_path = get_search_db_path()
    if not db_path: 
        return None
    
    current_mtime = os.path.getmtime(db_path)
    
    # æª”æ¡ˆæœ‰æ›´æ–°ï¼Œæˆ–æ˜¯ç¬¬ä¸€æ¬¡è¼‰å…¥æ™‚ï¼Œæ‰é‡æ–°è®€å– (å„ªåŒ–æ•ˆèƒ½)
    if _search_cache is None or current_mtime != _search_mtime:
        _search_mtime = current_mtime
        try: _search_cache = pd.read_csv(db_path, dtype=str, encoding='utf-8-sig')
        except:
            try: _search_cache = pd.read_csv(db_path, dtype=str, encoding='big5')
            except:
                try: _search_cache = pd.read_excel(db_path, dtype=str)
                except: return None
        
        # ğŸ’¡ã€æ¥µé€Ÿå„ªåŒ–ã€‘ï¼šé å…ˆæŠŠæ‰€æœ‰æ¬„ä½åˆä½µæˆä¸€å€‹å°å¯«æœå°‹æ¬„ä½
        if _search_cache is not None:
            _search_cache = _search_cache.fillna("")
            _search_cache['_combined_search_text'] = _search_cache.astype(str).agg(' '.join, axis=1).str.lower()
                
    return _search_cache

# ================= 1. ç²å–è³‡æ–™åº«è³‡è¨Š =================
@router.get("/info")
async def get_search_info():
    db_path = get_search_db_path()
    if not db_path:
        return {"total_records": 0, "current_db_name": "å°šæœªè¼‰å…¥"}
    
    df = load_search_db()
    if df is not None:
        display_name = os.path.basename(db_path)
        if os.path.exists(SEARCH_DB_NAME_FILE):
            with open(SEARCH_DB_NAME_FILE, "r", encoding="utf-8") as f:
                display_name = f.read().strip()
                
        return {"total_records": len(df), "current_db_name": display_name}
    return {"total_records": 0, "current_db_name": "æª”æ¡ˆæ ¼å¼éŒ¯èª¤"}

# ================= 2. ä¸Šå‚³æ›´æ–°è³‡æ–™åº« =================
@router.post("/upload")
async def upload_search_db(file: UploadFile = File(...)):
    try:
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in ['.csv', '.xlsx', '.xls']: file_ext = '.csv'
        save_path = os.path.join(DATA_DIR, f"search_data{file_ext}")
        
        # æ¸…ç†èˆŠæª”æ¡ˆ
        for ext in ['.csv', '.xlsx', '.xls']:
            old_file = os.path.join(DATA_DIR, f"search_data{ext}")
            if os.path.exists(old_file): os.remove(old_file)

        content = await file.read()
        with open(save_path, "wb") as f:
            f.write(content)
            
        # ç´€éŒ„åŸå§‹æª”å
        with open(SEARCH_DB_NAME_FILE, "w", encoding="utf-8") as f:
            f.write(file.filename)
            
        # å¼·åˆ¶æ¸…ç©ºå¿«å–
        global _search_cache, _search_mtime
        _search_cache = None
        _search_mtime = 0
            
        return {"message": "æœå°‹å°ˆç”¨è³‡æ–™åº«å·²æˆåŠŸæ›´æ–°ï¼"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# ================= 3. åŸ·è¡Œæœå°‹ (å« HKTVmall æ™ºæ…§é€£çµ) =================
@router.get("/")
async def search_barcode(q: str = Query(..., min_length=1)):
    df = load_search_db()
    if df is None:
        raise HTTPException(status_code=400, detail="è«‹å…ˆä¸Šå‚³è³‡æ–™åº«æª”æ¡ˆ")
    
    query_lower = str(q).lower()
    
    # ğŸ’¡ å‘é‡åŒ–æœå°‹ï¼šæ¯«ç§’ç´šç¯©é¸å‰ 50 ç­†
    matched_df = df[df['_combined_search_text'].str.contains(query_lower, na=False)].head(200)
    
    results = []
    for _, row in matched_df.iterrows():
        product_code = row.get("ProductCode", row.get("Product_No", ""))
        barcode_val = str(row.get("Barcode", "")).strip()
        name_val = str(row.get("Name", row.get("Description", ""))).strip()
        
        # ğŸŒŸ æ™ºæ…§é€£çµç”¢ç”Ÿé‚è¼¯
        search_url = row.get("SearchUrl", "").strip()
        
        if not search_url:
            if name_val and name_val.upper() != "NAN":
                encoded_name = urllib.parse.quote(name_val)
                search_url = f"https://www.hktvmall.com/hktv/zh/search_a?keyword={encoded_name}"
            else:
                search_url = "#" # çœŸçš„æ²’è³‡æ–™å°±çµ¦ç©ºé€£çµ

        results.append({
            "ProductCode": product_code,
            "Barcode": barcode_val if barcode_val.upper() != "NAN" else "",
            "Name": name_val if name_val.upper() != "NAN" else "ç„¡åç¨±",
            "SearchUrl": search_url
        })
            
    # æ‰“å¡ç´€éŒ„
    log_action("Barcode_Search")
    
    return results